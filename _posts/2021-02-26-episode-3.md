---
layout: post
title: "Our Image Recognition Models Are Too Vulnerable: Strategies and Defenses Against Adversarial Attacks | Ep. 3"
categories: episodes
---

## Listen Here
<html>
  <iframe src="https://anchor.fm/andre-ye/embed/episodes/Ep--3--Our-Image-Recognition-Models-Are-Too-Vulnerable-Strategies-and-Defenses-Against-Adversarial-Attacks-er50d7" height="102px" width="400px" frameborder="0" scrolling="no"></iframe>
</html>

## Description & Information

### Paper #1: "Explaining and Harnessing Adversarial Examples"
Access [here](https://arxiv.org/abs/1412.6572v3){:target="_blank"}
- This paper introduces the Fast Gradient Signed Method (FGSM) as a simple way to peturb a linear model in a way that significantly changes its output.
> The cause of these adversarial examples was a mystery, and speculative explanations have suggested it is due to extreme nonlinearity of deep neural networks, perhaps combined with insufficient model averaging and insufficient regularization of the purely supervised learning problem. We show that these speculative hypotheses are unnecessary. Linear behavior in high-dimensional spaces is sufficient to cause adversarial examples.

- Main contribution: whereas others have hypothesized that adversarial attacks may be the result of extreme nonlinearity, the authors prove that nonlinearity and generalization allow for adversarial attacks.
  - It follows, then, that models that are easy to train (i.e. get good performance easily) are also easy to be attacked adversarily.
- Results when applying FGSM to peturb linear models: phenomenon of high error rates *and* high confidence.
  - Shallow classifier to have an error rate of 99.9% with an average confidence of 79.3%. 
  - Another network misclassifies 89.4% of adversarial examples with an average confidence of 97.6%. 
- Adversarial training lowers error rate from 89.4% to 17.9% - seems to make the model somewhat resistant to adversarial examples.

### Paper #2: "Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks"
Access [here](https://people.cs.vt.edu/~gangwang/class/cs6604/papers/07546524.pdf){:target="_blank"}
- Papernot et al. introduce distillation as a defense against adversarial attacks.
- Distilattion training begins with training on hard labels (1 or 0), then training a second model to predict the output probabilities of the first model.
> Our intuition is that knowledge extracted by distillation, in the form of probability vectors, and transferred in smaller networks to maintain accuracies comparable with those of larger networks can also be beneficial to improving generalization capabilities of DNNs outside of their training dataset and therefore enhances their resilience to perturbations.

### Paper #3: "Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks"
Access [here](https://arxiv.org/abs/1704.01155){:target="_blank"}
- Xu et al. propose *feature squeezing* as a method to defend against adversarial attacks.
- Feature squeezing reduces the search space available to the adversary.
- Examples of feature squeezing include reducing the color bit depth of each pixel and spatial smoothing.

### Paper #4: "Towards Deep Learning Models Resistant to Adversarial Attacks"
Access [here](https://arxiv.org/pdf/1706.06083.pdf){:target="_blank"}
- Madry et al. formulate the adversarial problem as two problems:
  - Maximization problem: finding adversarial examples. What is a data point `x` that achieves a high loss?
  - Minimization problem: finding parameters such that the "adversarial loss" is minimized.
- The minimization problem is tied to the maximization problem; if we can answer the maximization problem, we can train our model against the maximization problem to derive the solution to the minimization problem.
- Projected Gradient Descent attempts to find the maxima of the loss, and is proven to be a "universal" attacker.
- By outlining PGD as a universal attacker, it follows that a model resistant to PGD is resistant to all other attacks.
- This explicit formulation of an attacker allows for a more structured framework to develop defenses in.



