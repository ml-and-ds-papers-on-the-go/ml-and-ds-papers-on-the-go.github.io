---
layout: post
title: "Deep Learning's Million-Dollar Question Pt. 1: Rethinking Generalization and Why We Should Care | Ep. 1"
categories: episodes
---

## Listen Here
<html>
  <iframe src="https://anchor.fm/andre-ye/embed/episodes/Deep-Learnings-Million-Dollar-Question-Pt--1-Rethinking-Generalization-and-Why-We-Care--Ep--1-eqa00d/a-a4iv72p" height="102px" width="400px" frameborder="0" scrolling="no"></iframe>
</html>

## Description & Information
Why don't neural networks overfit when they have so many parameters? It's deep learning's million-dollar question and one we'll try to get a few steps closer to answering. We'll get an introduction to the million-dollar question, get an understanding for the surprising amount of information neural networks can carry, and explore the discussion and research around Deep Double Descent phenomena.

*Links to the papers discussed:*
- <a href="https://arxiv.org/pdf/1611.03530.pdf">Understanding Deep Learning Requires Rethinking Generalization</a>: arxiv.org/pdf/1611.03530.pdf
- <a href="https://arxiv.org/pdf/1812.11118.pdf">Reconciling modern machine learning practice and the bias-variance trade-off</a>: arxiv.org/pdf/1812.11118.pdf
- <a href="https://arxiv.org/pdf/1901.01608.pdf">Scaling description of generalization with number of parameters in deep learning</a>: arxiv.org/pdf/1901.01608.pdf
