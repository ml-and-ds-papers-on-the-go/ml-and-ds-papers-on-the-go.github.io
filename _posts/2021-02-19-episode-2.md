---
layout: post
title: "Deep Learning’s Million Dollar Question Pt. 2: Pruning, Lottery Tickets, and High-Performance Random Networks | Ep. 2"
categories: episodes
---

## Listen Here
<html>
  <iframe src="https://anchor.fm/andre-ye/embed/episodes/Deep-Learnings-Million-Dollar-Question-Pt--2-Pruning--Lottery-Tickets--and-High-Performance-Random-Networks--Ep--2-eqlj2v" height="120px" width="600px" frameborder="0" scrolling="no"></iframe>
</html>

## Description & Information

### Paper #1: "Learning both Weights and Connections for Efficient Neural Networks"
Access [here](https://arxiv.org/pdf/1506.02626.pdf){:target="_blank"}
- This paper introduces the idea of *pruning* - the idea that not all connections are important, and those connections that are not important can be removed.
- The paper outlines pruning as a three-step process:
  1. Identify which connections are important. The authors suggest doing this by looking at the weight *value*.
  2. Prune unimportant connections by masking them.
  3. Fine-tune (continue training the network) with the new architecture.
- Significant results:
  - VGG16 trained on a ImageNet dataset; the pruned network was reduced to 7.5% its original size (13x smaller) with minimal loss in training.
  - AlexNet model (61m params) could be pruned to 1/9 of its original size without impacting accuracy; computation requirements were reduced by 3x.

### Paper #2: "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
Access [here](https://arxiv.org/pdf/1803.03635v4.pdf){:target="_blank"}
> ...dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that—when trained in isolation— reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.

- The Lottery Ticket Hypothesis states that neural networks operate giant lotteries, where each "lottery ticket" is a subnetwork.
- Winning tickets are differentiated from losing tickets depending on their initialization.
- If a winning ticket is isolated from the rest of the network, its weights are restored, and it is retrained, that winning ticket can reach a performance as high as the original network in a similar number of iterations.

### Paper #3: "What’s Hidden in a Randomly Weighted Neural Network?"
Access [here](https://arxiv.org/pdf/1911.13299.pdf){:target="_blank"}
- This paper extends the Lottery Ticket Hypothesis by stating that *within a random neural network lies a subnetwork that can perform as well as that original neural network, trained to completion*.
- The "edge popup" algorithm is used to identify winning tickets in the random neural network. It is similar to training a neural network, but instead of updating parameters, it updates scores used to determine which connections get pruned.
  1. For each edge (connection), assign a score.
  2. Use the wedges corresponding to the top *k*% of scores only - the important ones - in the forward propagation.
  3. During backpropagation, update all the scores.
