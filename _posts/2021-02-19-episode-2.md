---
layout: post
title: "Deep Learningâ€™s Million Dollar Question Pt. 2: Pruning, Lottery Tickets, and High-Performance Random Networks | Ep. 2"
categories: episodes
---

## Listen Here
<html>
  <iframe src="https://anchor.fm/andre-ye/embed/episodes/Deep-Learnings-Million-Dollar-Question-Pt--1-Rethinking-Generalization-and-Why-We-Care--Ep--1-eqa00d/a-a4iv72p" height="102px" width="400px" frameborder="0" scrolling="no"></iframe>
</html>

## Description & Information
Why don't neural networks overfit when they have so many parameters? It's deep learning's million-dollar question and one we'll try to get a few steps closer to answering. We'll get an introduction to the million-dollar question, get an understanding for the surprising amount of information neural networks can carry, and explore the discussion and research around Deep Double Descent phenomena.

*Links to the papers discussed:*
